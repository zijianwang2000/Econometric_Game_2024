ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)
censored_model = censReg(censored_y ~ x)
summary(censored_model)
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
set.seed(123)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=0)
cbind(y, censored_y)
# Fit the uncensored regression model
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit the linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   #
# Fit the censored regression model
censored_model = censReg(censored_y ~ x, left=0)
summary(censored_model)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)
cbind(y, censored_y)
# Fit the uncensored regression model
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit the linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   #
# Fit the censored regression model
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
install.packages("sampleSelection")
### Sample selection model
library(sampleSelection)
set.seed(123)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)
cbind(y, censored_y)
# Fit the uncensored regression model
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit the linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   # inconsistent
# Fit the censored regression model
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)
cbind(y, censored_y)
# Fit the uncensored regression model
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit the linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   # inconsistent
# Fit the censored regression model
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
set.seed(123)
n = 100
x = rnorm(n)   # covariates
t = rbinom(n, 1, 0.5)
u = rnorm(n)   # error term for wage equation
y_star = 1 + 2*x + 0.5*t + u # latent outcome
y = ifelse(y_star>0, y_star, 0)
set.seed(123)
n = 100
x = rnorm(n)   # covariates
t = rbinom(n, 1, 0.5)
u = rnorm(n)   # error term for wage equation
y_star = 1 + 2*x + 0.5*t + u # latent outcome
y = ifelse(y_star>0, y_star, 0)
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
summary(heckman_model)
library(mlogit)
data("Fishing", package = "mlogit")
Fish = dfidx(Fishing, varying=2:9, shape="wide", choice="mode")
#str(Fish)
m = mlogit(mode ~ price + catch | income, data=Fish)
summary(m)
View(Fishing)
library(censReg)
set.seed(321)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)
cbind(y, censored_y)
# Fit the uncensored regression model
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit the linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   # inconsistent
# Fit the censored regression model
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
set.seed(321)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)   # c=1
cbind(y, censored_y)
uncensored_model = lm(y ~ x)
summary(uncensored_model)
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
probit_model = glm(y ~ x, family=binomial(link="probit"))
library(MASS)
library(ggplot2)
# Generate random data
set.seed(321)
n = 100   # Number of observations
x = runif(n)
# P(y=1|x)=Phi(2x)
y = rbinom(n, 1, pnorm(2*x))
# Fit linear model (misspecified)
linear_model = lm(y ~ x)
summary(linear_model)
# Fit probit model
probit_model = glm(y ~ x, family=binomial(link="probit"))
summary(probit_model)
set.seed(321)
n = 100   # Number of observations
x = runif(n)
# P(y=1|x)=Phi(2x)
y = rbinom(n, 1, pnorm(2*x))
# Fit linear model (misspecified)
linear_model = lm(y ~ x)
summary(linear_model)
# Fit probit model
probit_model = glm(y ~ x, family=binomial(link="probit"))
summary(probit_model)
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
set.seed(321)
n = 100   # sample size
x = rnorm(n)   # covariates
# beta = (1,2), u~N(0,1)
y = rnorm(n, mean=1+2*x)   # latent outcomes
censored_y = y * as.numeric(y>=1)   # c=1
cbind(y, censored_y)
# Fit linear regression model on uncensored data (cheating)
uncensored_model = lm(y ~ x)
summary(uncensored_model)
# Fit linear regression model ignoring censoring
ig_censored_model = lm(censored_y ~ x)
summary(ig_censored_model)   # inconsistent
# Fit the correct censored regression model
censored_model = censReg(censored_y ~ x, left=1)
summary(censored_model)
library(sampleSelection)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
set.seed(123)
n = 100
x = rnorm(n)   # covariates
t = rbinom(n, 1, 0.5)   # selection indicator
u = rnorm(n)   # error term for outcome equation
y_star = 1 + 2*x + 0.5*t + u   # latent outcome
y = ifelse(y_star>0, y_star, 0) # can reformulate selection indicator!!!
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
summary(heckman_model)
cbind(y,t)
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
summary(heckman_model)
library(sampleSelection)
# Generate data
set.seed(123)
n = 100
x = rnorm(n)   # covariates
t = rbinom(n, 1, 0.5)   # selection indicator
u = rnorm(n)   # error term for outcome equation
y_star = 1 + 2*x + 0.5*t + u   # latent outcome
y = ifelse(y_star>0, y_star, 0) # can reformulate selection indicator!!!
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
summary(heckman_model)
# often in
library(mvtnorm)
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, 1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, -0.2, -0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
uw
library(sampleSelection)
library(mvtnorm)
# Generate data
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(-1 - 3*z >= w)   # selection indicator, gamma=(-1,-3)
y = ifelse(t=1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
w
y
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(x >= w)   # selection indicator, gamma=(-1,-3)
y = ifelse(t=1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ x)
w
x
z
y
library(sampleSelection)
# Generate data
set.seed(123)
n = 100
x = rnorm(n)   # covariates
t = rbinom(n, 1, 0.5)   # selection indicator
u = rnorm(n)   # error term for outcome equation
y_star = 1 + 2*x + 0.5*t + u   # latent outcome
y = ifelse(y_star>0, y_star, 0) # can reformulate selection indicator!!!
y
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t=1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y_star ~ x, selection = t ~ z)
summary(heckman_model)
y = ifelse(t=1, y_star, 0)
y
y_star
t
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
### Sample selection model (own example)
library(sampleSelection)
library(mvtnorm)
# Generate data
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z <= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
sum(t)
library(sampleSelection)
library(mvtnorm)
# Generate data
set.seed(123)
n = 100
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
set.seed(123)
n = 150
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
set.seed(123)
n = 200
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
set.seed(123)
n = 150
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 + 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
selected = sum(t)
library(sampleSelection)
library(mvtnorm)
# Generate data
set.seed(123)
n = 150
x = rnorm(n)   # covariates
z = 0.7*x + rnorm(n, -1, 0.5)   # IV
mean_vec <- c(0, 0)
cov_mat <- matrix(c(0.36, 0.2, 0.2, 1), nrow = 2)
uw = rmvnorm(n, mean_vec, cov_mat)
u = uw[1:n, 1]   # error term for outcome equation
w = uw[1:n, 2]   # error term for selection equation
y_star = 1 - 2*x + u   # latent outcome, beta=(1,2)
t = as.numeric(z >= w)   # selection indicator, gamma=(0,1)
selected = sum(t)
y = ifelse(t==1, y_star, 0)
# Heckman two-stage
heckman_model = heckit(outcome = y ~ x, selection = t ~ z)
summary(heckman_model)
selected
install.packages("gmm")
library(gmm)
moment_cond = function(theta, data){
x = data$x
y = data$y
moment1 = (y - theta[1] - theta[2] * x)
moment2 = (y - theta[1] - theta[2] * x) * x
moment3 = (y - theta[1] - theta[2] * x)^2 - theta[3]
return(cbind(moment1, moment2, moment3))
}
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, .7)
y = 2 + 3 * x + e
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
library(gmm)
# Model: y = beta_0 + x*beta_1 + e, exogeneity, sigma² = E(e²)
# Moments: E[y-beta_0-beta_1*x]=0, E[(y-beta_0-beta_1*x)x]=0, E[(y-beta_0-beta_1*x)²-sigma²]=0
moment_cond = function(theta, data){
x = data$x
y = data$y
moment1 = (y - theta[1] - theta[2] * x)
moment2 = (y - theta[1] - theta[2] * x) * x
moment3 = (y - theta[1] - theta[2] * x)^2 - theta[3]
return(cbind(moment1, moment2, moment3))
}
# Generate data
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
View(data)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(0,0,2))   # starting value for optimization
summary(gmm_model)
set.seed(321)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(0,0,2))   # starting value for optimization
summary(gmm_model)
set.seed(1)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(0,0,2))   # starting value for optimization
summary(gmm_model)
gmm_model = gmm(moment_cond, data, t0=c(-5,5,20))   # starting value for optimization
summary(gmm_model)
set.seed(1)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(-5,5,20))   # starting value for optimization
summary(gmm_model)
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(-5,5,20))   # starting value for optimization
summary(gmm_model)
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
# Estimate GMM
gmm_model = gmm(moment_cond, data, t0=c(0,0,3))   # starting value for optimization
summary(gmm_model)
lm_model = lm(y ~ x)
summary(lm_model)
library(gmm)
# Model: y = beta_0 + x*beta_1 + e, exogeneity, sigma² = E(e²)
# Moments: E[y-beta_0-beta_1*x]=0, E[(y-beta_0-beta_1*x)x]=0, E[(y-beta_0-beta_1*x)²-sigma²]=0
moment_cond = function(theta, data){
x = data$x
y = data$y
moment1 = (y - theta[1] - theta[2] * x)
moment2 = (y - theta[1] - theta[2] * x) * x
moment3 = (y - theta[1] - theta[2] * x)^2 - theta[3]
return(cbind(moment1, moment2, moment3))
}
# Generate data
set.seed(123)
n = 100
x = rnorm(n)
e = rnorm(n, 0, sqrt(.5))
y = 2 + 3 * x + e
# Want to estimate theta = (2, 3, 0.5)
data = data.frame(x=x, y=y)
lm_model = lm(y ~ x)
summary(lm_model)
gmm_model = gmm(moment_cond, data, t0=c(0,0,2))   # starting value for numerical optimization
summary(gmm_model)
gmm_model = gmm(moment_cond, data, t0=c(0,0,2), wmatrix="ident")
summary(gmm_model)
